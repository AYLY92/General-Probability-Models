---
title: 'TP3: Bayésien naif'
output:
  html_document:
    df_print: paged
  pdf_document: default
---
### 1. Donner les estimateurs du max de vrai de tous les params


$$
On \, sait \, que: \,  \forall \, k \in \, \{1, 2\} \, et \, \forall \, j \in \, \{1, 2\} \, X^j \, / \, \{Y = k\} \leadsto N(\mu_{kj}, \sigma_{kj}^2):  \\

D'où: 
f_{kj} = \frac{1}{\sigma_{kj\sqrt{2\pi}}}\exp((\frac{-1}{2})
(\frac{x-\mu_{kj}}{\sigma_{kj}})^2) \\
L(X_{1},..., X_{n}, \mu_{kj}, \sigma_{kj}) = \prod_{i=1}^n[
(\frac{1}{\sigma_{kj}\sqrt{2\pi}})\exp((\frac{-1}{2})(\frac{x-\mu_{kj}}{\sigma_{kj}})^2)]\\
\Rightarrow \boxed{L(X_{1},..., X_{n}, \mu_{kj}, \sigma_{kj})  = \prod_{i=1}^n[(2\pi\sigma_{kj}^2)^\frac{-1}{2}\exp((\frac{-1}{2})(\frac{x_{i} -\mu_{kj}}{\sigma_{kj}})^2)]}\\
l(X_{1},..., X_{n}, \mu_{kj}, \sigma_{kj}) = logL(X_{1},..., X_{n}, \mu_{kj}, \sigma_{kj})\\
\Rightarrow \boxed{l(X_{1},..., X_{n}, \mu_{kj}, \sigma_{kj}) = log(2\pi\sigma^2_{kj})^\frac{-n}{2} - \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})^2}{2\sigma^2_{kj}}}\\
On \,a:\,\frac{\delta ln[L]}{\delta\mu_{kj}} = 0  \Rightarrow -\frac{-2\sum_{i=1}^n(x_{i}-\mu_{kj})}{2\sigma^2_{kj}} = 0 \\
\Rightarrow \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})}{\sigma^2_{kj}} = 0 \\
\Rightarrow \mu_{kj} = \frac{\sum_{i=1}^nx_{i}}{n}\\
Donc:\, \boxed{\widehat{\mu}_{kj} = \overline{X}} \\
On\, a: \, \frac{\delta ln[L]}{\delta\sigma^2} = 0 \\
\Rightarrow (\frac{-n}{2}.\frac{1}{\sigma^2_{kj}})+ \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})^2}{2\sigma^4_{kj}} = 0\\
\Rightarrow \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})^2}{2\sigma^4_{kj}} = \frac{n}{2\sigma^2_{kj}} \\
\Rightarrow \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})^2}{\sigma^2_{kj}} = n\\

\Rightarrow \sigma^2_{kj} = \frac{\sum_{i=1}^n(x_{i}-\mu_{kj})^2}{n}\\
Donc: \, \boxed{\widehat \sigma^2_{kj} = \frac{\sum_{i=1}^n(X_{i}-\overline{X})^2}{n}}\\
On \,a:\,\frac{\delta^2 ln[L]}{\delta\mu_{kj}^2} = - \frac{\sum_{i=1}^nx_{i}}{\sigma} < 0 \\
Et: \frac{\delta^2 ln[L]}{\delta\sigma_{kj}^2} = \frac{-8\sigma^3_{kj} \sum_{i=1}^n(x_{i}-\mu_{kj})^2}{4(\sigma^4_{kj})^2} < 0 \\\
Comme\, que\, les\, dérivées\, secondes\, sont\, négatives \,\\ alors\, \widehat\mu_{kj}\, et\, \widehat\sigma^2_{kj}\,
sont\, les\, estimateurs\, du\, maximum\, de\, vraisemblance\, de\, \\ \mu_{kj}\, et\, \sigma^2_{kj}. 
Ils \, représentent\, respectivement\, la\, moyenne \\
et\, la\, variance\, empirique\, de\, l'échantillon.
$$

### 3. Chargement du jeux de données 
```{r}
synth_train <- read.delim("D:/UFR/PGM/TP/synth_train.txt")
head(synth_train)
```
### Transforamtion de Y en facteur
```{r}
synth_train$y = factor(synth_train$y)
class(synth_train$y)
```

### 4. Implémentation de la méthode du bayésien naif
##### a. La fonction d'estimation bn_estim
```{r}
bn_estim <- function(data){
  
  ## P(x1/{y=1})
  mu_11 <- mean(data[data$y==1, "x1"])
  std_11 <- sd(data[data$y==1, "x1"])
  ## P(x2/{y=1})
  mu_12 <- mean(data[data$y==1, "x2"])
  std_12 <- sd(data[data$y==1, "x2"])
  
  ## P(x1/{y=2})
  mu_21 <- mean(data[data$y==2, "x1"])
  std_21 <- sd(data[data$y==2, "x1"])
  ## P(x2/{Y=2})
  mu_22 <- mean(data[data$y==2, "x2"])
  std_22 <- sd(data[data$y==2, "x2"])
  
  return(c(mu_11, std_11, mu_12, std_12, mu_21, std_21, mu_22, std_22))
}

```

#### b. La fonction de prédiction bn_predict
```{r}
bn_predict = function(data, test){
  ## appel de la ftn bn_estim
  params =  bn_estim(data) 
  ## initialisation à de la classe de la variable à prédire à 0
  yp = 0
  for (i in 1:dim(test)[1]) {
    ## modalités de x1 et x2
    x1 = test$x1[i]
    x2 = test$x2[i]
    ## Densités conditionelles de f1 et f2
    f1 = exp(-(x1-params[1])^2)/sqrt(2*pi*params[2]^2) *       exp(-(x2-params[3])^2)/sqrt(2*pi*params[4]^2)
    f2 = exp(-(x1-params[5])^2)/sqrt(2*pi*params[6]^2) *       exp(-(x2-params[7])^2)/sqrt(2*pi*params[8]^2)
    
    if(f1>f2){
      yp[i] = 1
    }
    else{
      yp[i] = 2
    }
  }
  yp
}

```

#### 5. Testons les fonctions:
##### Application de bn_estim sur les données de train

```{r}
yp = bn_predict(synth_train, synth_train)
print(yp)
```

#### Application de bn_predict sur les points de coordonnées (0,1) et (-2,2)

```{r}
## Création d'un dataframe contenant les deux points
x1 = c(0, -2)
x2 = c(1, 2)
df = data.frame(x1, x2)
## prédiction
yval = bn_predict(synth_train, df)
```

```{r}
print(paste0("Prediction du point de coordonnees (0,1) : yval = ", yval[1]))

```

```{r}
print(paste0("Prediction du point de coordonnees (-2,2) : yval = ", yval[2]))

```

### 6. Calcul du taux d'erreur d'apprentissage
```{r}
## matrice de confusion
confmat = table(synth_train$y, yp)
## erreur
erreur = 1 - sum(diag(confmat)) / sum(confmat)
print(paste0("Taux d'erreur d'apprentissage : ", format(round(erreur*100, 2), nsmall = 2), "%"))
```
### 7. Chargement des données de test
```{r}
synth_test = read.delim("D:/UFR/PGM/TP/synth_test.txt")
head(synth_test)
```

#### Transforamtion de Y en facteur 
```{r}
synth_test$y = factor(synth_test$y)
class(synth_test$y)
```
#### Calcul du taux d'erreur de test
```{r}
## prédiction de y sur les données de test
ytestp = bn_predict(synth_train, synth_test)
## matrice de confusion
confmat = table(synth_test$y, ytestp)
## erreur
erreur = 1 - sum(diag(confmat)) / sum(confmat)
print(paste0("Taux d'erreur de test : ", format(round(erreur*100, 2), nsmall = 2), "%"))
```





















